{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install praw\nfrom spacy.matcher import Matcher\nimport pandas as pd\nimport spacy\nimport praw\n#pd.options.display.float_format = '{:.5f}'.format","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"reddit = praw.Reddit(client_id='74UKCjQe_A-udw', client_secret='-1jNIfc3BM8eVr4b0zZXy74Xwag', user_agent='Project Spikes')\nelections = reddit.subreddit('MtAugusta').search('title:[Election]', limit=150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build a DataFrame with those posts\nposts = pd.DataFrame([[post.title, post.url, post.score, post.num_comments, post.created] for post in elections if post.title[:10].lower() == '[election]'],\n                     columns=['title', 'url', 'score', 'num_comments', 'created'])\nposts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define a get_comments function that returns all comments from a given URL\ncomments = pd.DataFrame()\ndef get_comments(url):\n    submission = reddit.submission(url=url)\n    for comment in submission.comments:\n        try:\n            comments.loc[comment.author.name, url] = '[' + comment.body + ']'\n        except AttributeError:\n            continue\n    return comments\n#For example:\nfor post in posts.index:\n    get_comments(posts.url[post])\ncomments.to_csv('comments.csv')\ncomments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\n#Build a matcher for numbers followed by punctuation\nmatcher = Matcher(nlp.vocab)\nmatcher.add('UserMat', None,\n            [{\"POS\": \"NUM\"}, {\"IS_PUNCT\": True}, {\"LIKE_NUM\": False}])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IRRELEVANT: used for debugging\ndef tokenize(doc):\n    for token in nlp(doc):\n        print(token, token.pos_)\n\n#Takes a username and counts every vote said user has casted\ndef count_votes(username):\n    output = {}\n    for comment in comments.loc[username].dropna():\n        doc = nlp(str(comment).replace('\\\\n', ' ').replace('\\\\t', ' ').replace(')', '.'))\n        matches = matcher(doc)\n        \n        for match_id, start, end in matches:\n            string_id = nlp.vocab.strings[match_id]\n            span = doc[start:end]\n            if span[:1].text == '1':\n                try:\n                    output[span[2:].text.lower()] = output[span[2:].text.lower()] + 5\n                except KeyError:\n                    output[span[2:].text.lower()] = 5\n            elif span[:1].text == '2':\n                try:\n                    output[span[2:].text.lower()] = output[span[2:].text.lower()] + 3\n                except KeyError:\n                    output[span[2:].text.lower()] = 3\n            else:\n                try:\n                    output[span[2:].text.lower()] = output[span[2:].text.lower()] + 2\n                except KeyError:\n                    output[span[2:].text.lower()] = 2\n    return output\n\ntest = count_votes('squareblob')\nprint(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a df with votes per candidate by voter\ndf = pd.DataFrame(index=comments.index)\nfor voter, _ in df.iterrows():\n    ballots = count_votes(voter)\n    for candidate in ballots.keys():\n        df.loc[voter, candidate] = ballots[candidate]\ndf.to_csv('affinity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a df with voter participation per election (0 if not voted, 1 if voted)\nparticipation = comments.T.notnull().astype('int')\nfor col in participation.columns:\n    if participation[col].sum() < 4:\n        participation.drop(col, axis=1, inplace=True)\nparticipation.to_csv('participation.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_votes('squareblob')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Same behaviour as count_votes but takes a URL instead\ndef count_votes_url(url):\n    output = {}\n    for comment in comments[url].dropna():\n        doc = nlp(str(comment).replace('\\\\n', ' ').replace('\\\\t', ' ').replace(')', '.'))\n        matches = matcher(doc)\n        \n        for match_id, start, end in matches:\n            string_id = nlp.vocab.strings[match_id]\n            span = doc[start:end]\n            if span[:1].text == '1':\n                try:\n                    output[span[2:].text.lower()] = output[span[2:].text.lower()] + 5\n                except KeyError:\n                    output[span[2:].text.lower()] = 5\n            elif span[:1].text == '2':\n                try:\n                    output[span[2:].text.lower()] = output[span[2:].text.lower()] + 3\n                except KeyError:\n                    output[span[2:].text.lower()] = 3\n            else:\n                try:\n                    output[span[2:].text.lower()] = output[span[2:].text.lower()] + 2\n                except KeyError:\n                    output[span[2:].text.lower()] = 2\n    return output\ncount_votes_url('https://www.reddit.com/r/MtAugusta/comments/hd1437/election_judge_iii/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a df2 with votes per candidate by post\ndf2 = pd.DataFrame(index=posts.url)\nfor url in df2.index:\n    ballots = count_votes_url(url)\n    for candidate in ballots.keys():\n        df2.loc[url, candidate] = ballots[candidate]\ndf2.to_csv('popularity.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"posts.set_index('url', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for url in df2.index:\n    df2.loc[url, 'time'] = posts.created[url]\ndf2['time'] = df2['time'].astype(int)\ndf2.sort_values(by='time', inplace=True)\ndf2['time'] = pd.to_datetime(df2['time'], unit='s')\ndf2.fillna(0, inplace=True)\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testframe = pd.DataFrame({'A': [1,2,3,4,5,6], 'B':[0,1,0,1,0,1], 'C':[0,0,0,0,0,0]})\n\n#Need a function to sum merge columns to avoid data fragmentation\ndef mergesum(df, parent, child):\n    for row in range(df[parent].size):\n        df[parent][row] = df[parent][row] + df[child][row]\n    df.drop(child, axis=1, inplace=True)\n    \ndef mergesums(df, parent, childs):\n    for child in childs:\n        try:\n            mergesum(df, parent, child)\n        except KeyError:\n            continue\nmergesums(testframe, 'A', ['B','C'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mergesums(df2, 'citylion', ['city', 'citypedeleon', 'citylion1'])\nmergesums(df2, 'programmc', ['program', 'dchero', 'wrongaardvark', '\\u2060programmc', \"d'chero\"])\nmergesums(df2, 'ahrimanne', ['ahri', 'ahrimanazu', 'pamplamanazu', ''])\nmergesums(df2, 'topher3001', ['topher', 'topha', 'toph'])\nmergesums(df2, 'sandfalls', ['swissandham', 'swiss', 'swiss&ham'])\nmergesums(df2, 'waffle4breakfast', ['waffle', 'waffles', 'pancake5lunch', 'crepe6dinner'])\nmergesums(df2, 'rakkwal', ['rakk', 'rakkwall'])\nmergesums(df2, 'kingcold64', ['kingcold', 'fladeedledoodle'])\nmergesums(df2, 'henrydraton', ['henry', 'henrydration'])\nmergesums(df2, 'godomasta', ['godo', '\\u2060godomasta', 'godomasto', 'gododmasta'])\nmergesum(df2, 'squareblob', 'square')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2cum = df2.iloc[:,:-1]\ndf2cum = df2cum.cumsum()\ndf2cum['time'] = df2['time'].dt.date\ndf2cum.set_index('time', inplace=True)\ndf2cum.to_csv('popularity.csv')\npd.set_option('max_columns', None)\ndf2cum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FINALLY! Start plotting the racing bars\n!pip install bar_chart_race\nimport bar_chart_race as bcr\nstartdate = pd.to_datetime(\"2018-04-16\").date()\nenddate = pd.to_datetime(\"2019-12-08\").date()\nrace = df2cum.loc[startdate:enddate]\nbcr.bar_chart_race(race, n_bars=10, steps_per_period=60, period_length=800)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}